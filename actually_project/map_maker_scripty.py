# -*- coding: utf-8 -*-
"""vgg_saliency.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13jcTEhoAtzd8_pEq8SEJjzs5RsGoXIiD
"""

# from google.colab import drive
# drive.mount('/content/drive/')

import numpy as np
import os
import tensorflow as tf
# from tensorboardcolab import *
import scipy
import scipy.stats as st
import cv2

# !pip3 install opencv-python
tf.reset_default_graph()

# tf.test.gpu_device_name() # should print '/device:GPU:0' if Runtime is properly set to GPU

WEIGHTS_PATH = '../cv2_data/vgg16-conv-weights.npz'
DATA_PATH = '../cv2_data/'
MODEL_PATH = '../cv2_data/models/'
RESULTS_PATH = '../cv2_data/results_angelie_kraft_anton_volkov/'

# tf.reset_default_graph()

weights = np.load(WEIGHTS_PATH)
for k in weights.keys():
  print(k + 'shape: {}'.format(weights[k].shape))

def create_dataset_from_files(files, grey=False):
    def load_img(img):
      np_image = cv2.imread(img)
      if grey:
        np_image= cv2.cvtColor(np_image, cv2.COLOR_BGR2GRAY)
        np_image = np.expand_dims(np_image, axis=-1).astype(np.float32)
      return np_image
    listing = os.listdir(files)
    res = np.array([load_img(os.path.join(files, img)) for i, img in enumerate(listing)]) # every 5th image
    return res


# cv2_data_path = '/content/drive/My Drive/CV/cv2_data/' # replace with your path
# cv2_training_imgs = os.path.join(DATA_PATH,'train/images')
# cv2_training_fixs = os.path.join(DATA_PATH,'train/fixations')
cv2_validation_imgs = os.path.join(DATA_PATH, 'val/images')
cv2_validation_fixs = os.path.join(DATA_PATH, 'val/fixations')
cv2_testing = os.path.join(DATA_PATH, 'test/images')

# print('Loading train_X')
# train_X = create_dataset_from_files(cv2_training_imgs)
# print(train_X.shape, train_X.dtype)

# print('Loading train_y')
# train_y = create_dataset_from_files(cv2_training_fixs, True)
# print(train_y.shape, train_y.dtype)

print('Loading validation_X')
validation_X = create_dataset_from_files(cv2_validation_imgs)
print(validation_X.shape, validation_X.dtype)

print('Loading validation_y')
validation_y = create_dataset_from_files(cv2_validation_fixs, True)
print(validation_y.shape, validation_y.dtype)

print('Loading test_X')
test_X = create_dataset_from_files(cv2_testing)
print(test_X.shape, test_X.dtype)

images = tf.placeholder(tf.uint8, [1, 180, 320, 3]) # None to indicate a dimension can vary at runtime
gaussian = tf.placeholder(tf.float32, [None, 45, 80, 1])

# labels = tf.placeholder(tf.int64, [None, 180, 320, 1])

def gaussian2d():
    x = np.linspace(0, 1, 45+1)
    y = np.linspace(0, 1, 80+1)
    gaussian1d_x = np.diff(st.norm.cdf(x, loc=0.5, scale=0.2))
    gaussian1d_y = np.diff(st.norm.cdf(y, loc=0.5, scale=0.2))
    gaussian2d = np.outer(gaussian1d_x, gaussian1d_y).reshape((-1, 45, 80, 1))
    return gaussian2d

with tf.name_scope('preprocess') as scope:
  imgs = tf.image.convert_image_dtype(images, tf.float32) * 255.0
  mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32
                       , shape=[1 , 1, 1, 3], name='img_mean')
  imgs_normalized = imgs - mean

with tf.name_scope('conv1_1') as scope:
	kernel = tf.Variable(initial_value=weights['conv1_1_W'], trainable=True, name="weights")
	biases = tf.Variable(initial_value=weights['conv1_1_b'], trainable=True, name="biases")
	conv = tf.nn.conv2d(imgs_normalized, kernel, [1, 1, 1, 1], padding='SAME')
	out = tf.nn.bias_add(conv, biases)
	act = tf.nn.relu(out, name=scope)

with tf.name_scope('conv1_2') as scope:
	kernel = tf.Variable(initial_value=weights['conv1_2_W'], trainable=True, name="weights")
	biases = tf.Variable(initial_value=weights['conv1_2_b'], trainable=True, name="biases")
	conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')
	out = tf.nn.bias_add(conv, biases)
	act = tf.nn.relu(out, name=scope)

with tf.name_scope('pool1') as scope:
	pool1 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')

with tf.name_scope('conv2_1') as scope:
	kernel = tf.Variable(initial_value=weights['conv2_1_W'], trainable=True, name="weights")
	biases = tf.Variable(initial_value=weights['conv2_1_b'], trainable=True, name="biases")
	conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')
	out = tf.nn.bias_add(conv, biases)
	act = tf.nn.relu(out, name=scope)

with tf.name_scope('conv2_2') as scope:
	kernel = tf.Variable(initial_value=weights['conv2_2_W'], trainable=True, name="weights")
	biases = tf.Variable(initial_value=weights['conv2_2_b'], trainable=True, name="biases")
	conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')
	out = tf.nn.bias_add(conv, biases)
	act = tf.nn.relu(out, name=scope)

with tf.name_scope('pool2') as scope:
	pool2 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')

with tf.name_scope('conv3_1') as scope:
	kernel = tf.Variable(initial_value=weights['conv3_1_W'], trainable=True, name="weights")
	biases = tf.Variable(initial_value=weights['conv3_1_b'], trainable=True, name="biases")
	conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')
	out = tf.nn.bias_add(conv, biases)
	act = tf.nn.relu(out, name=scope)

with tf.name_scope('conv3_2') as scope:
	kernel = tf.Variable(initial_value=weights['conv3_2_W'], trainable=True, name="weights")
	biases = tf.Variable(initial_value=weights['conv3_2_b'], trainable=True, name="biases")
	conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')
	out = tf.nn.bias_add(conv, biases)
	act = tf.nn.relu(out, name=scope)

with tf.name_scope('conv3_3') as scope:
	kernel = tf.Variable(initial_value=weights['conv3_3_W'], trainable=True, name="weights")
	biases = tf.Variable(initial_value=weights['conv3_3_b'], trainable=True, name="biases")
	conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')
	out = tf.nn.bias_add(conv, biases)
	act = tf.nn.relu(out, name=scope)

with tf.name_scope('pool3') as scope:
	pool3 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')
  
with tf.name_scope('conv4_1') as scope:
	kernel = tf.Variable(initial_value=weights['conv4_1_W'], trainable=True, name="weights")
	biases = tf.Variable(initial_value=weights['conv4_1_b'], trainable=True, name="biases")
	conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')
	out = tf.nn.bias_add(conv, biases)
	act = tf.nn.relu(out, name=scope)

with tf.name_scope('concat') as scope:
  out = tf.concat([pool2, pool3, act], -1)

#with tf.name_scope('dropout') as scope:
 # out = tf.keras.layers.Dropout(rate=1-0.5)(out)
  
with tf.name_scope('conv_sal_1') as scope:
  init = tf.initializers.glorot_normal()
  kernel = tf.Variable(init([3, 3, 896, 64]), name="kernel_1")
  biases = tf.Variable(tf.zeros([64,], tf.float32))
  conv = tf.nn.conv2d(out, kernel, [1, 1, 1, 1], padding='SAME')
  out = tf.nn.bias_add(conv, biases)
  act = tf.nn.relu(out, name=scope)
  
with tf.name_scope('conv_sal_2') as scope:
  init = tf.initializers.glorot_normal()
  kernel = tf.Variable(init([1, 1, 64, 1]), name="kernel_2")
  biases = tf.Variable(tf.zeros([1,], tf.float32))
  conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')
  out = tf.nn.bias_add(conv, biases)
  saliency_raw = tf.nn.relu(out, name=scope)

with tf.name_scope('gaussian') as scope:
  shape = tf.convert_to_tensor([-1, 45, 80, 1])
  gaussian = tf.reshape(tf.convert_to_tensor(gaussian), shape)
  saliency_raw = tf.math.multiply(saliency_raw, gaussian, name=None)
  max_value_per_image = tf.reduce_max(saliency_raw, axis=[1,2,3], keepdims=True)
  predicted_saliency = (saliency_raw / max_value_per_image)


gaussian2d = gaussian2d() 



import matplotlib.pyplot as plt

# saver = tf.train.import_meta_graph(os.path.join(MODEL_PATH, 'trained_model-1.meta'))
saver = tf.train.Saver()
# for tensor in tf.get_default_graph().get_operations():
#     print (tensor.name)
# from tensorflow.python.training import checkpoint_utils as cp
# print(cp.list_variables('C:/Users/anton/Documents/Study/CV2_project/cv2_data/models/trained_model-1'))
test_filenames = ["".join((os.path.splitext(img)[0], '_prediction.jpg')) for img in os.listdir(cv2_testing)]

with tf.Session() as sess:
# #   # writer = tf.summary.FileWriter(logdir="./", graph=sess.graph)
  sess.run(tf.global_variables_initializer())
  saver.restore(sess, os.path.join(MODEL_PATH, 'trained_model-267'))
#   print('Model restored y√§y')
# # #   print('conv_sal_1/kernel_1 = {}'.format(kernel.eval()) )
  for i in range(len(test_X[:5])):
    res = sess.run(predicted_saliency, feed_dict={images: np.expand_dims(test_X[i], 0), gaussian: gaussian2d})
    img = tf.reshape(res, [45, 80, 1])
    img = tf.image.resize(img, [120, 320])
    img = tf.image.convert_image_dtype(img, tf.uint8)
    img_jpeg = tf.image.encode_jpeg(img, format='grayscale')
    img_path = os.path.join(RESULTS_PATH, test_filenames[i])
    print(img_path)
    tf.io.write_file(img_path, img_jpeg.eval(), name="file_writer")
    
    
